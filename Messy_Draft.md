# Abstract

Technical documentation is boring. It's boring to create, it's boring to find, and it's especially boring to read. But, technical documentation is also an incredibly important resource for users and developers of digital libraries. This is especially true of large scale digital library initiatives that offer application programming interfaces (APIs) for accessing complex, large corpora of digital objects. In this paper we propose evaluation criteria for APIs

## Documentation as a DL evaluation challenges




### Why does documentation matter? 

Complete and timely technical documentation has been show to be the leading indicator of trust between developers and users of open-source software (Dagenais & Robillar, 2010). 

There is a long history of studying technical documentation in terms of its understandability, and its effectiveness for end users. For instance, an economic study conducted for the USA ARMY estimated that improved technical documentation could save the government $1.7 million (Shriver and Hart, 1975). 

Q & A sites, development blogs, and social media also play an important role in supplementing existing documentation - in some cases - offering high levels of coverage and author engagement (Parnin and Treude, 2011). A distinction can be made between the types of documentation we've described thus far: 

- *creator documentaiton*

- *communnity documentation*, such as wikis used by open source projects, allow anyone involved with the creation or use of a resource to contribute to documentation about the resource. 

- *crowd documentation*, such as developer blogs and Q and A, are individual contributions that often focus on documenting a single aspect of the resource. In aggregate, crowd documentation has proven to be rich in both coverage and quality [2011]


Our focus is on documentation for application programming interfaces (APIs) in digital libraries. 


-- WHAT ARE APIs AND WHY ARE THEY IMPORTANT 

APIs are thus an important aspect of end-user interactions with a digital library. API's, and technical documentation more generally, are not part of the evaluation process for digital library development. This paper seeks to fill this gap in two ways: 

1. Provide a background to API usability, documentation, and evaluation from software industry
2. Propose a metric which quantifies the coverage and completeness of API documentation, both formal and informal. 
3. Evaluate five large-scale digital library APIs based on these criteria. 



# Background 

- Talk about big DL initiatives here
- Talk about interoperability here
- Talk about what the paper will do here.... 

# Related work 

Tools for API documentation: Jungloids and Jadeite


# Research Questions 

# Methods

## Data Collection

Create a list of number of API methods available across the DLs

## Data Analysis



## Works Cited

- B.Dagenais and M. P. Robillard. Creating and
evolving developer documentation: Understanding the
decisions of open source contributors. In
Proceedings
of the 18th ACM SIGSOFT International Symposium
on the Foundations of Software Engineering
, pages
127–136, November 2010

-  D. Mandelin, L. Xu, R. Bod ́ık, and D. Kimelman.
Jungloid mining: helping to navigate the api jungle. In
Proceedings of the 2005 ACM SIGPLAN conference
on Programming language design and implementation
,
PLDI ’05, pages 48–61. ACM, 2005

-  J. Stylos, B. A. Myers, and Z. Yang. Jadeite:
improving api documentation using usage information.
In
Proceedings of the 27th international conference
extended abstracts on Human factors in computing
systems
, CHI ’09, pages 4429–4434, New York, NY,
USA, 2009. ACM

- Shriver, E. and Hart, F. Study and proposal for the improvement of military technical
information transfer methods. Aberdeen Proving Grounds, MD: U.S. Army Human Engineering
Laboratory, December 1975


Other things to look at: 

- Metadata Coverage Index: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3558968/